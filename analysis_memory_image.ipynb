{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from itertools import izip\n",
    "import pandas as pd\n",
    "path='/home/gaurav/malware_combine_result/Results_Malware/result_'\n",
    "final_dict='/home/gaurav/malware_combine_result/Results_clean/final_dict'\n",
    "\n",
    "feature=[]\n",
    "\n",
    "\n",
    "dic_final=open(final_dict,\"a\")\n",
    "for i in range(0,27):\n",
    "    \n",
    "    fp=path+str(i)\n",
    "   \n",
    "    with open(os.path.join(fp,'dict1'),'r')as a,open(os.path.join(fp,'dll1'),'r')as b,open(os.path.join(fp,'envars'),'r')as c,open(os.path.join(fp,'file_names'),'r')as d,open(os.path.join(fp,'mutex'),'r')as e:\n",
    "              \n",
    "              \n",
    "              count=0  \n",
    "              for p,q,r,s,t in izip(a,b,c,d,e):\n",
    "                    dict_1={}\n",
    "                    p = eval(p.strip())\n",
    "                    q = eval(q.strip())\n",
    "                    r = eval(r.strip())\n",
    "                    s = eval(s.strip())\n",
    "                    t = eval(t.strip())\n",
    "                    \n",
    "                    dict_1=dict(dict_1.items()+p.items())\n",
    "                    dict_1=dict(dict_1.items()+q.items())\n",
    "                    dict_1=dict(dict_1.items()+r.items())\n",
    "                    dict_1=dict(dict_1.items()+s.items())\n",
    "                    dict_1=dict(dict_1.items()+t.items())\n",
    "                    feature.append(dict_1)\n",
    "                    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=pd.DataFrame.from_records(feature).fillna(0)\n",
    "list1=[]\n",
    "df0['possible_code_injection']= df0['possible_code_injection']/(df0['no._of_child']+1)#@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "for names in df0:\n",
    "     if df0[names].std() > 0.2:\n",
    "            print names\n",
    "            list1.append(names)\n",
    "            print df0[names].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/gaurav/malware_combine_result/Results_clean/result_'\n",
    "\n",
    "feature1=[]\n",
    "\n",
    "\n",
    "\n",
    "count=0 \n",
    "for i in range(0,23):\n",
    "    \n",
    "    fp=path+str(i)\n",
    "    #print i\n",
    "    #print '#########################################################'\n",
    "    \n",
    "    with open(os.path.join(fp,'dict1'),'r')as a,open(os.path.join(fp,'dll1'),'r')as b,open(os.path.join(fp,'envars'),'r')as c,open(os.path.join(fp,'file_names'),'r')as d,open(os.path.join(fp,'mutex'),'r')as e:\n",
    "              #print 'hello'\n",
    "              \n",
    "              \n",
    "              for p,q,r,s,t in izip(a,b,c,d,e):\n",
    "                    dict_1={}\n",
    "                    p = eval(p.strip())\n",
    "                    q = eval(q.strip())\n",
    "                    r = eval(r.strip())\n",
    "                    s = eval(s.strip())\n",
    "                    t = eval(t.strip())\n",
    "                    \n",
    "                    dict_1=dict(dict_1.items()+p.items())\n",
    "                    dict_1=dict(dict_1.items()+q.items())\n",
    "                    dict_1=dict(dict_1.items()+r.items())\n",
    "                    dict_1=dict(dict_1.items()+s.items())\n",
    "                    dict_1=dict(dict_1.items()+t.items())\n",
    "                    \n",
    "                    \n",
    "                   \n",
    "                    #print dict_1\n",
    "                    feature1.append(dict_1)\n",
    "\n",
    "                    count+=1\n",
    "                    #if count>490:\n",
    "                        #break\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    #if count>490:\n",
    "            #break              \n",
    "                    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "print len(feature1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame.from_records(feature1).fillna(0)\n",
    "list2=[]\n",
    "df1['possible_code_injection']= df1['possible_code_injection']/(df1['no._of_child']+1)\n",
    "for names in df1:\n",
    "     if df1[names].std() > 0.2:\n",
    "            #print names\n",
    "            list2.append(names)\n",
    "            #print df1[names].sum()\n",
    "            if names in df0:\n",
    "                print names+' '+str(df0[names].sum())+' '+str(df1[names].sum())\n",
    "\n",
    "print df1['possible_code_injection'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list3=[]\n",
    "for names in df0:\n",
    "     if df0[names].std() > 0.2:\n",
    "            #print names\n",
    "            \n",
    "            #print df1[names].sum()\n",
    "            if names in df1:\n",
    "                print names+' '+str(df0[names].sum())+' '+str(df1[names].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list3=list1+list2\n",
    "print list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list3=list1+list2\n",
    "print list3\n",
    "del list3[list3.index('Can bypass UAC ')]\n",
    "#del list3[list3.index('Can create atoms ')]\n",
    "del list3[list3.index('Can create or start services ')]\n",
    "del list3[list3.index('Can create or write to files ')]\n",
    "del list3[list3.index('Can enumerate free disk space ')]\n",
    "del list3[list3.index('Can enumerate system information ')]\n",
    "del list3[list3.index('Can identify machine time ')]\n",
    "del list3[list3.index('Can identify machine version information ')]\n",
    "del list3[list3.index('Can inject code to other processes ')]\n",
    "del list3[list3.index('Can interact with the registry ')]\n",
    "del list3[list3.index('Can query startup information ')]\n",
    "del list3[list3.index('Can receive or send files from or to internet ')]\n",
    "del list3[list3.index('Can track keyboard strokes ')]\n",
    "del list3[list3.index('Can use antidebug techniques ')]\n",
    "\n",
    "\n",
    "\n",
    "print list3\n",
    "'''\n",
    "Can bypass UAC  0 58\n",
    "Can create atoms  0 20\n",
    "Can create or start services  0 29\n",
    "Can create or write to files  0 162\n",
    "Can enumerate free disk space  0 36\n",
    "Can enumerate system information  0 181\n",
    "Can identify machine time  0 89\n",
    "Can identify machine version information  0 94\n",
    "Can inject code to other processes  0 240\n",
    "Can interact with the registry  0 123\n",
    "Can query startup information  0 44\n",
    "Can receive or send files from or to internet  0 323\n",
    "Can track keyboard strokes  0 31\n",
    "Can use antidebug techniques  0 444\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict={}\n",
    "for name in list3:\n",
    "    final_dict[name]=0\n",
    "print final_dict\n",
    "final_dict['Y']=0\n",
    "print len(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from itertools import izip\n",
    "import pandas as pd\n",
    "path='/home/gaurav/malware_combine_result/Results_Malware/result_'\n",
    "\n",
    "\n",
    "\n",
    "feature_all=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,27):\n",
    "    \n",
    "    fp=path+str(i)\n",
    "  \n",
    "    with open(os.path.join(fp,'dict1'),'r')as a,open(os.path.join(fp,'dll1'),'r')as b,open(os.path.join(fp,'envars'),'r')as c,open(os.path.join(fp,'file_names'),'r')as d,open(os.path.join(fp,'mutex'),'r')as e:\n",
    "              #print 'hello'\n",
    "              \n",
    "              count=0  \n",
    "              for p,q,r,s,t in izip(a,b,c,d,e):\n",
    "                    dict_1={}\n",
    "                    final_dict1={}\n",
    "                   \n",
    "                    final_dict1=final_dict.copy()\n",
    "                    p = eval(p.strip())\n",
    "                    q = eval(q.strip())\n",
    "                    r = eval(r.strip())\n",
    "                    s = eval(s.strip())\n",
    "                    t = eval(t.strip())\n",
    "                    \n",
    "                    dict_1=dict(dict_1.items()+p.items())\n",
    "                    dict_1=dict(dict_1.items()+q.items())\n",
    "                    dict_1=dict(dict_1.items()+r.items())\n",
    "                    dict_1=dict(dict_1.items()+s.items())\n",
    "                    dict_1=dict(dict_1.items()+t.items())\n",
    "                    \n",
    "                    \n",
    "                   \n",
    "                    for name in dict_1.keys():\n",
    "                        if name in final_dict.keys():\n",
    "                            final_dict1[name]=dict_1[name]\n",
    "                    final_dict1['Y']= 1\n",
    "                    feature_all.append(final_dict1)\n",
    "#print feature_all  \n",
    "\n",
    "print len(feature_all)\n",
    "\n",
    "\n",
    "path='/home/gaurav/malware_combine_result/Results_clean/result_'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count=0 \n",
    "\n",
    "for i in range(0,23):\n",
    "    \n",
    "    fp=path+str(i)\n",
    "   \n",
    "    \n",
    "    with open(os.path.join(fp,'dict1'),'r')as a,open(os.path.join(fp,'dll1'),'r')as b,open(os.path.join(fp,'envars'),'r')as c,open(os.path.join(fp,'file_names'),'r')as d,open(os.path.join(fp,'mutex'),'r')as e:\n",
    "              #print 'hello'\n",
    "              \n",
    "              \n",
    "              for p,q,r,s,t in izip(a,b,c,d,e):\n",
    "                    dict_1={}\n",
    "                    final_dict1={}\n",
    "                    final_dict1=final_dict.copy()\n",
    "                    p = eval(p.strip())\n",
    "                    q = eval(q.strip())\n",
    "                    r = eval(r.strip())\n",
    "                    s = eval(s.strip())\n",
    "                    t = eval(t.strip())\n",
    "                    \n",
    "                    dict_1=dict(dict_1.items()+p.items())\n",
    "                    dict_1=dict(dict_1.items()+q.items())\n",
    "                    dict_1=dict(dict_1.items()+r.items())\n",
    "                    dict_1=dict(dict_1.items()+s.items())\n",
    "                    dict_1=dict(dict_1.items()+t.items())\n",
    "                    \n",
    "                    \n",
    "                   \n",
    "                    for name in dict_1.keys():\n",
    "                        if name in final_dict.keys():\n",
    "                            final_dict1[name]=dict_1[name]\n",
    "                    final_dict1['Y']=0\n",
    "                    feature_all.append(final_dict1)\n",
    "\n",
    "                    count+=1\n",
    "                    #print count\n",
    "                    #if count>490:\n",
    "                        #break\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    #if count>490:\n",
    "            #break              \n",
    "                    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7=pd.DataFrame.from_records(feature_all).fillna(0)\n",
    "print df7.shape\n",
    "print df7.var().sort_values()\n",
    "'''\n",
    "df7=df7.drop('Mutants_accessed',axis=1)\n",
    "df7=df7.drop('Keylogger traces (messagehooks)',axis=1)\n",
    "df7=df7.drop('\\nHas access to autorun registry keys',axis=1)\n",
    "df7=df7.drop('interesting files accessed',axis=1)\n",
    "df7=df7.drop('\\nCollects information about system',axis=1)\n",
    "df7=df7.drop('no._of_child',axis=1)\n",
    "df7=df7.drop('Susp DLLs/EXEs (dlllist)',axis=1)\n",
    "df7=df7.drop('possible_code_injection',axis=1)\n",
    "df7=df7.drop('dll_for_proc',axis=1)\n",
    "df7=df7.drop('suspicious_files',axis=1)\n",
    "df7=df7.drop('hidden/Susp DLLs/EXEs(ldermodules)',axis=1)\n",
    "df7=df7.drop('Associated service ',axis=1)\n",
    "df7=df7.drop('count_run_temp_dir',axis=1)\n",
    "df7=df7.drop('\\nReads information about supported languages',axis=1)\n",
    "df7=df7.drop('IPcount netscan',axis=1)\n",
    "df7=df7.drop('no._of_envars',axis=1)\n",
    "df7=df7.drop('no. of malfinds',axis=1)\n",
    "df7=df7.drop('\\nQueries / modifies proxy settings',axis=1)\n",
    "df7=df7.drop('new services',axis=1)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import timeit\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import svm\n",
    "df3=df7\n",
    "print df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########pCA-------####################\n",
    "for k in range(1,2,1):\n",
    "    val=0\n",
    "    for n in range(1,20):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.metrics import precision_recall_fscore_support\n",
    "        \n",
    "        train, test = train_test_split(df3, test_size = 0.3)\n",
    "        X_train=pd.DataFrame(train)\n",
    "\n",
    "        Y_train=list(X_train['Y'])#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        \n",
    "                        \n",
    "        \n",
    "        X_train=X_train.drop('Y',axis=1)\n",
    "        #X_train=pd.DataFrame(X_train[n])\n",
    "        X_train = StandardScaler().fit_transform(X_train)\n",
    "\n",
    "\n",
    "        X_test=pd.DataFrame(test)\n",
    "        #print len(X_train)\n",
    "\n",
    "        Y_test=list(X_test['Y'])\n",
    "        X_test=X_test.drop('Y',axis=1)\n",
    "        #X_test=pd.DataFrame(X_test[n])\n",
    "\n",
    "        X_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "        #print len(X_test)   \n",
    "        ##############################################\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "        # fits PCA, transforms data and fits the decision tree classifier\n",
    "        # on the transformed data\n",
    "        #pipe = Pipeline([('pca', PCA(n_components=1)),\n",
    "        #                 ('tree', DecisionTreeClassifier())])\n",
    "        \n",
    "\n",
    "        #pipe.fit(X_train, Y_train)\n",
    "        #pca = PCA(20)\n",
    "\n",
    "        #pca.fit(X_train)\n",
    "        #print(pca.components_)\n",
    "        #print(pca.explained_variance_)\n",
    "        #print pipe.score(X_test,Y_test)\n",
    "        #val=val+ pipe.score(X_test,Y_test)\n",
    "        clf = DecisionTreeClassifier(random_state=0)\n",
    "        #clf=svm.SVC()\n",
    "        #clf = clf.fit(X_train,Y_train)\n",
    "            #print clf.feature_importances_\n",
    "        \n",
    "        clf = clf.fit(X_train,Y_train)\n",
    "        Y_pred = clf.predict(X_test)\n",
    "        #clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "        \n",
    "            #print clf.feature_importances_\n",
    "\n",
    "        #Y_pred = clf.predict(X_test)\n",
    "        \n",
    "        print clf.score(X_test,Y_test)\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        from mlxtend.plotting import plot_confusion_matrix\n",
    "        cm = confusion_matrix(Y_test, \n",
    "                              Y_pred)\n",
    "        fig, ax = plot_confusion_matrix(conf_mat=cm)\n",
    "        plt.figure(figsize=(1,1))\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "        '''\n",
    "        if clf.score(X_test,Y_test)>0.7:\n",
    "                import numpy as np\n",
    "                import pandas as pd\n",
    "                print n\n",
    "                print clf.score(X_test,Y_test)\n",
    "                print precision_recall_fscore_support(Y_test, Y_pred, average='macro')\n",
    "                print df0[n].mean()\n",
    "                print df1[n].mean()\n",
    "                d = {n+'\\n one' : df0[n],'two' : df1[n]}\n",
    "                     \n",
    "\n",
    "\n",
    "               \n",
    "\n",
    "                df = pd.DataFrame(d)\n",
    "                df.plot(style=['o','rx'],figsize=(20,10))\n",
    "                df7=df7.drop(n,axis=1)\n",
    "    #print 'k value:' +str(k) +'   '+str(val/30)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "cm = confusion_matrix(Y_test, \n",
    "                      Y_pred)\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm)\n",
    "plt.figure(figsize=(1,1))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    print corr_matrix\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if corr_matrix.iloc[i, j] >= threshold:\n",
    "                colname = corr_matrix.columns[i] # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "                print colname\n",
    "                if colname in dataset.columns:\n",
    "                    \n",
    "                    if colname=='Y':\n",
    "                        continue\n",
    "                    else:\n",
    "                       \n",
    "                        del dataset[colname] # deleting the column from the dataset\n",
    "\n",
    "    print col_corr\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df7\n",
    "mylist=correlation(df3,0.9)\n",
    "#print\n",
    "#print mylist\n",
    "\n",
    "#print mylist['Y']\n",
    "for name1 in mylist:\n",
    "    print name1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mylist=mylist.drop('BootstrapperApplicationData.xml',axis=1)\n",
    "mylist=mylist.drop('Devdiv',axis=1)\n",
    "mylist=mylist.drop('System.Data.dll',axis=1)\n",
    "mylist=mylist.drop('',axis=1)\n",
    "mylist=mylist.drop('Associated service ',axis=1)\n",
    "mylist=mylist.drop('Susp DLLs/EXEs (dlllist)',axis=1)\n",
    "\n",
    "mylist=mylist.drop('\\nCollects information about system',axis=1)\n",
    "mylist=mylist.drop('Unusual timers.',axis=1)\n",
    "mylist=mylist.drop('SearchFilterHo',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1,2):\n",
    "    \n",
    "\n",
    "            train, test = train_test_split(mylist, test_size = 0.3)\n",
    "            #print X_train\n",
    "            X_train=pd.DataFrame(train)\n",
    "            \n",
    "\n",
    "            Y_train=list(X_train['Y'])\n",
    "            #print Y_train\n",
    "            \n",
    "            \n",
    "\n",
    "            X_train=X_train.drop('Y',axis=1)\n",
    "\n",
    "\n",
    "            X_test=pd.DataFrame(test)\n",
    "            \n",
    "\n",
    "            Y_test=list(X_test['Y'])\n",
    "            X_test=X_test.drop('Y',axis=1)\n",
    "\n",
    "            #for name in X_train :\n",
    "            #      print name\n",
    "            #print len(X_test)   \n",
    "            clf = DecisionTreeClassifier(random_state=0)\n",
    "            #clf=svm.SVC()\n",
    "            clf = clf.fit(X_train,Y_train)\n",
    "            #print clf.feature_importances_\n",
    "\n",
    "            Y_pred = clf.predict(X_test)\n",
    "           # print(\"accuracy on the training subset: \",clf.score(X_train,Y_train))\n",
    "            #print(\"accuracy on the testing subset: \",clf.score(X_test,Y_test))\n",
    "            #print Y_test[:200]\n",
    "            #print Y_pred[:200]            \n",
    "            for i in range(0,len(Y_pred)):\n",
    "                if Y_pred[i]==Y_test[i]:\n",
    "                    continue\n",
    "                else :\n",
    "                    print 'F'+str(Y_test[i])+' '+str(Y_pred[i])\n",
    "            print clf.score(X_test,Y_test)\n",
    "            coun=0\n",
    "            '''for name in mylist:\n",
    "                    print name+'   '+str(coun)\n",
    "                    coun=coun+1'''\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toshow=[]\n",
    "for na in X_train:\n",
    "    featurevals={'name':na,\n",
    "                 'mean M':df0[na].mean(),\n",
    "                 'Sd M':df0[na].std(),\n",
    "                 'mean B':df1[na].mean(),\n",
    "                 'Sd B'  :df1[na].std() \n",
    "                    \n",
    "                }\n",
    "    #print na+' '+str(df0[na].mean())+' '+str(df0[na].std())+' '+str(df1[na].mean())+' '+str(df1[na].std())\n",
    "    toshow.append(featurevals)\n",
    "print toshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showdf=pd.DataFrame().from_records(toshow).fillna(0).round(3)\n",
    "print showdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "print clf.feature_importances_\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(\"accuracy on the training subset: \",clf.score(X_train,Y_train))\n",
    "print(\"accuracy on the testing subset: \",clf.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from pandas_ml import ConfusionMatrix\n",
    "cm = ConfusionMatrix(Y_test, Y_pred) \n",
    "print cm                    \n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm)\n",
    "plt.figure(figsize=(1,1))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, Y_train)  \n",
    "print(\"accuracy on the testing subset: \",clf.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  \n",
    "classifier = KNeighborsClassifier(n_neighbors=5)  \n",
    "classifier.fit(X_train, Y_train)  \n",
    "y_pred = classifier.predict(X_test)  \n",
    "\n",
    "print(confusion_matrix(Y_test, y_pred))  \n",
    "print(classification_report(Y_test, y_pred))  \n",
    "print classifier.score(X_test,Y_test)\n",
    "cm = confusion_matrix(Y_test, \n",
    "                      y_pred)\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm)\n",
    "plt.figure(figsize=(1,1))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model no training data\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = XGBClassifier()\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "#print(confusion_matrix(Y_test, y_pred))  \n",
    "#print(classification_report(Y_test, y_pred))  \n",
    "#print classifier.score(X_test,Y_test)\n",
    "cm = confusion_matrix(Y_test, \n",
    "                      y_pred)\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm)\n",
    "plt.figure(figsize=(1,1))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
